{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15666ce8-5bca-405c-8cc3-3ba1cace8fe1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T15:46:01.761224Z",
     "iopub.status.busy": "2025-03-18T15:46:01.760843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing for connection type: SPARK_GLUE, connection name: project.spark.compatibility\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The following configurations have been updated: {'session_type': 'streaming'}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Glue session...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Session 4cmygnn241c1zv-d12ec11c-f515-4121-8567-d93b1abb6cdf has been created.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <table class=\"session_info_table\" style=\"width: 75%; margin-top: var(--jp-content-heading-margin-top); margin-bottom:var(--jp-content-heading-margin-bottom); border: var(--jp-border-width) solid var(--jp-border-color2);\">\n",
       "                    <tr>\n",
       "                        <th style=\"text-align: left; border: var(--jp-border-width) solid var(--jp-border-color2);\">Id</th>\n",
       "                        <th style=\"text-align: left; border: var(--jp-border-width) solid var(--jp-border-color2);\">Spark UI</th>\n",
       "                        <th style=\"text-align: left; border: var(--jp-border-width) solid var(--jp-border-color2);\">Driver logs</th>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td class=\"application_id\" style=\"word-wrap: break-word; text-align: left; border: var(--jp-border-width) solid var(--jp-border-color2)\">4cmygnn241c1zv-d12ec11c-f515-4121-8567-d93b1abb6cdf</td>\n",
       "                        <td class=\"spark_ui_link\" style=\"word-wrap: break-word; text-align: left; border: var(--jp-border-width) solid var(--jp-border-color2)\"><a href=\"\" target=\"_blank\" log_location=\"s3://amazon-sagemaker-825765423553-sa-east-1-dac29af4cfb9/dzd_cjfgk2iz50fxp7/4cmygnn241c1zv/dev/glue/glue-spark-events-logs/\">link</a></td>\n",
       "                        <td class=\"driver_log_link\" style=\"word-wrap: break-word; text-align: left; border: var(--jp-border-width) solid var(--jp-border-color2)\"><a href=\"\" target=\"_blank\" log_location=\"s3://amazon-sagemaker-825765423553-sa-east-1-dac29af4cfb9/dzd_cjfgk2iz50fxp7/4cmygnn241c1zv/dev/glue/glue-spark-system-logs/4cmygnn241c1zv-d12ec11c-f515-4121-8567-d93b1abb6cdf/driver/stderr.gz\">link</a></td>\n",
       "                    </tr>\n",
       "                </table>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session created for connection: project.spark.compatibility.\n"
     ]
    }
   ],
   "source": [
    "%%pyspark project.spark.compatibility\n",
    "%streaming -f\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.functions import when, lit\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "import boto3\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_secret(parameter_name):\n",
    "    \"\"\"Retrieve secret from AWS Parameter Store\"\"\"\n",
    "    try:\n",
    "        ssm = boto3.client('ssm', region_name='sa-east-1')  # adjust region as needed\n",
    "        response = ssm.get_parameter(\n",
    "            Name=parameter_name,\n",
    "            WithDecryption=True\n",
    "        )\n",
    "        return response['Parameter']['Value']\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error retrieving parameter {parameter_name}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Fetch credentials from Parameter Store\n",
    "try:\n",
    "    kds_arn = get_secret('/itau/kds/cdc_arn')\n",
    "    logger.info(\"Successfully retrieved database credentials from Parameter Store\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to retrieve credentials from Parameter Store\")\n",
    "    raise\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName(\"PostgreSQL to S3 ETL\") \\\n",
    "                    .config(\"spark.sql.catalog.glue_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "                    .config(\"spark.sql.catalog.glue_catalog.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n",
    "                    .config(\"spark.sql.catalog.glue_catalog.warehouse\", \"s3://amazon-sagemaker-825765423553-sa-east-1-dac29af4cfb9/iceberg_catalog/\") \\\n",
    "                    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "                    .config(\"spark.sql.catalog.glue_catalog.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
    "                    .config(\"spark.sql.iceberg.handle-timestamp-without-timezone\", True) \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "glueContext = GlueContext(sc)\n",
    "job = Job(glueContext)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"data\", StructType([\n",
    "        StructField(\"customer_id\", LongType()),\n",
    "        StructField(\"first_name\", StringType()),\n",
    "        StructField(\"last_name\", StringType()),\n",
    "        StructField(\"date_of_birth\", DateType()),\n",
    "        StructField(\"email\", StringType()),\n",
    "        StructField(\"phone_number\", StringType()),\n",
    "        StructField(\"employment_status\", StringType()),\n",
    "        StructField(\"annual_income\", LongType()),\n",
    "        StructField(\"created_at\", TimestampType()),\n",
    "        StructField(\"updated_at\", TimestampType())\n",
    "    ])),\n",
    "    StructField(\"metadata\", StructType([\n",
    "        StructField(\"timestamp\", StringType()),\n",
    "        StructField(\"record-type\", StringType()),\n",
    "        StructField(\"operation\", StringType()),\n",
    "        StructField(\"partition-key-type\", StringType()),\n",
    "        StructField(\"schema-name\", StringType()),\n",
    "        StructField(\"table-name\", StringType())\n",
    "    ]))\n",
    "])\n",
    "\n",
    "raw_df = glueContext.create_data_frame.from_options(\n",
    "    connection_type=\"kinesis\",\n",
    "    connection_options={\n",
    "        \"streamARN\": kds_arn,\n",
    "        \"initialPosition\": \"LATEST\",\n",
    "        \"inferSchema\": \"true\",\n",
    "        \"classification\": \"json\"\n",
    "    },\n",
    "    transformation_ctx=\"raw_df\"\n",
    ")\n",
    "\n",
    "def process_batch(df, epoch_id):\n",
    "    ssc_df = df.select(\n",
    "        from_json(col(\"$json$data_infer_schema$_temporary$\"), schema).alias(\"parsed_data\")\n",
    "    ).select(\"parsed_data.*\") \\\n",
    "    .select(\n",
    "        col(\"data.customer_id\").alias(\"customer_id\"),\n",
    "        col(\"data.first_name\").alias(\"first_name\"),\n",
    "        col(\"data.last_name\").alias(\"last_name\"),\n",
    "        col(\"data.date_of_birth\").alias(\"date_of_birth\"),\n",
    "        col(\"data.email\").alias(\"email\"),\n",
    "        col(\"data.phone_number\").alias(\"phone_number\"),\n",
    "        col(\"data.employment_status\").alias(\"employment_status\"),\n",
    "        col(\"data.annual_income\").alias(\"annual_income\"),\n",
    "        col(\"data.created_at\").alias(\"created_at\"),\n",
    "        col(\"data.updated_at\").alias(\"updated_at\"),\n",
    "        col(\"metadata.timestamp\").alias(\"event_timestamp\"),\n",
    "        col(\"metadata.record-type\").alias(\"record_type\"),\n",
    "        col(\"metadata.operation\").alias(\"operation\")\n",
    "    )\n",
    "    \n",
    "    ssc_df.createOrReplaceGlobalTempView('ssc_table')\n",
    "    \n",
    "    merge_sql = \"\"\"\n",
    "        MERGE INTO glue_db_aw53flfpa5qkyj.customer AS t\n",
    "        USING global_temp.ssc_table AS s\n",
    "        ON t.customer_id = s.customer_id\n",
    "        WHEN MATCHED AND s.operation = 'update' THEN\n",
    "            UPDATE SET\n",
    "                first_name = s.first_name,\n",
    "                last_name = s.last_name,\n",
    "                date_of_birth = s.date_of_birth,\n",
    "                email = s.email,\n",
    "                phone_number = s.phone_number,\n",
    "                employment_status = s.employment_status,\n",
    "                annual_income = s.annual_income,\n",
    "                created_at = s.created_at,\n",
    "                updated_at = s.updated_at\n",
    "        WHEN MATCHED AND s.operation = 'delete' THEN\n",
    "            DELETE\n",
    "        WHEN NOT MATCHED AND s.operation = 'insert' THEN\n",
    "            INSERT *\n",
    "    \"\"\"\n",
    "    \n",
    "    spark.sql(merge_sql)\n",
    "    spark.catalog.dropGlobalTempView(\"ssc_table\")\n",
    "\n",
    "final_df = raw_df \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .option(\"checkpointLocation\", \"s3://itau-sm-demo-825765423553/streaming_checkpoint\") \\\n",
    "    .trigger(processingTime=\"1 second\") \\\n",
    "    .start() \\\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93262d01-824a-4b5a-8bc9-3d019b3863b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark.compatibility\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
